# 失信人名单爬虫项目

## 项目需求

+ 1.1 抓取百度失信人名单
+ 1.2 抓取最高人民法院失信人名单
+ 1.3 抓取企业信用信息公示系统失信人名单
+ 1.4 把上面三个来源失信人信息进行合并,去重
  + 对于个人:
    + 根据证件号当做唯一标准
  + 对于企业:
    + 根据 区域 和 企业名称 当做唯一性标准

## 开发环境

* 开发语言: Python3
* 技术选择:
  * 爬虫框架: 使用 `scrapy` 
  * 数据解析: json,re
  * 持久化存储: mysql
  * 反爬处理: js2py



## 抓取失信人名单

 ### 实现步骤

* 创建爬虫项目
* 根据需求,定义数据模型
* 实现百度失信名单爬虫
* 实现随机UA和代理下载,解决ip反爬



### 创建爬虫项目

* `scrapy startprojec dishonest`

### 根据需求,定义数据模型

* 步骤:

  * 定义数据模型类: DishonestItem,继承 scrapy.Item
  * 定义要爬取字段
    * 失信人名称
    * 失信人号码
    * 失信人年年
    * 区域
    * 法人(企业)
    * 失信内容
    * 公布日期
    * 公布/执行单位
    * 创建日期
    * 更新日期

* 代码

 ```python
  class DishonestItem(scrapy.Item):
      #- 失信人名称
      name = scrapy.Field()
      #- 失信人证件号码
      card_num = scrapy.Field()
      #- 失信人年龄,企业为0
      age = scrapy.Field()
      #- 区域
      area = scrapy.Field()
      #- 法人(企业)
      business_entity = scrapy.Field()
      #- 失信内容
      conten = scrapy.Field()
      #- 公布日期
      publish_date = scrapy.Field()
      #- 公布/执行单位
      publish_unit = scrapy.Field()
      #- 创建日期
      create_date = scrapy.Field()
      #- 更新日期
      update_date = scrapy.Field()
 ```

### 实现百度失信名单爬虫

* 步骤:
  * 确定数据所在url
    * url:https://sp0.baidu.com/8aQDcjqpAAV3otqbppnN2DJv/api.php?resource_id=6899&query=失信人&pn=20&rn=10&ie=utf-8&oe=utf-8&format=json
    * 请求方法:GET
    * 参数/data:  &pn=n*10
    * 请求头Reference:https://www.baidu.com/s?ie=utf-8&rsv_idx=1&tn=baidu&wd={}'.format(parse.quote("失信人")
    * 返回数据的个数 50
    * 如何实现翻页
  * 创建爬虫
    * 实现
      *  创建爬虫`scrapy genspider baidu baidu.com`
  * 完善爬虫
    * 起始url 
    * 设置请求头,在 settings.py
    * 解析页面,提取需要的数据



### 保存失信人名单信息

* 步骤

  * mysql 
    * docker 中启动mysql
      * `docker run --name mysql8 -v $PWD/mysqldata:/var/lib/mysql -p 3300:3306 -e MYSQL_ROOT_PASSWORD=密码 -d mysql`
    * 进入mysql容器中
      * `docker exec -it mysql8 bash`
      * mysql
  * 创建数据,创建表
  * 在settings.py中配置数据库信息
  * 实现管道(Pipeline)类
  * 在settings.py中开启

* 实现

  * 创建数据库,表

    -- 创建数据库

    ```mysql
    create database dishonest  CHARACTER SET utf8 COLLATE utf8_general_ci;
    
    -- 创建表如果存在就先删除
    
    drop table if exists dishonest;
    use dishonest;
    -- 创建表
    create table dishonest(
    	dishonest_id INT NOT NULL AUTO_INCREMENT, -- id主键
    	age INT NOT NULL, -- 年龄,自然人年龄都是>0,企业是0
        name VARCHAR(200) NOT NULL, -- 人名 或 企业名.
        card_num VARCHAR(50), -- 身份证号码
        area VARCHAR(100) NOT NULL, -- 区域
        content VARCHAR(5000) NOT NULL , -- 内容
        business_entity VARCHAR(20) , -- 企业法人
        publish_unit VARCHAR(200), -- 发布单位
        publish_date VARCHAR(20), -- 发布时间
        create_date  DATETIME, -- 创建日期
        update_date DATETIME, -- 更新日期
        PRIMARY KEY (dishonest_id)
    );
    ```

* mysql8 授权访问(和之前不一样)
```mysql
create user 'admin'@'%' identified by 'Abcabc123...';
grant all privileges on dishonest to 'admin'@'%' ;
```


* 在settings.py中配置数据库信息
  
```python
  MYSQL_HOST = "192.168.3.103"
  MYSQL_PORT = 3306
  MYSQL_USER = "admin"
  MYSQL_PASSWORD = "Abcabc123..."
  MYSQL_DB = 'dishonest'
```

* 编写Pipeline(持久化数据)
* 开启pipeline



### 实现抓取最高人民法院失信人名单爬虫

* 分析页面,确地URL
* URL:http://jszx.court.gov.cn/api/front/getPublishInfoPageList
* 请求方法:POST
* 请求参数:
* 请求数据:"pageSize": 10,"pageNo": 1,
* 请求头:UA
* 相应的数据格式:json
* 实现翻页:pageNo. 最大页数来自 response.json['pageCount']

#### 完善爬虫

​	`scrapy genspider court court.gov.cn`

* 步骤
  * 构建起始URL
  * 获取总页数,构建所有页面请求
  * 解析页面数据(json)
* 代码



### 企业信用公示系统爬虫

* 目标: 从国家企业 信用公示系统的失信人公告中抓取失信企业信息
* 步骤:
  * 1. 分析页面,确定失信企业信息的请求
    2. 创建,完善爬虫

#### 分析,测试

* 步骤:

* http://www.gsxt.gov.cn/corp-query-entprise-info-xxgg-100000.html 获取区域id

  * 确定获取数据的rul  数据json接口 http://www.gsxt.gov.cn/affiche-query-area-info-paperall.html?noticeType=11&areaid=100000&noticeTitle=&regOrg=110000

  * 测试数据的请求 post

  * 请求参数  regOrg=110000 区域(省份id)

  * 请求数据 draw=1&start=0&length=10  draw 点击分页按钮次数 ,start 开始 条数,length 每页条数

  * 返回数据格式 json

  * 解决传递cookie问题

    * ```css
      __jsluid_h=0503d74e98d9754c18d08f5fb2f334e5; 
      SECTOKEN=6960446757962254479;
      UM_distinctid=16c141aa12518c-0d747d1b061843-e343166-15f900-16c141aa12651e;
      CNZZDATA1261033118=621392976-1563704006-http%253A%252F%252Fwww.gsxt.gov.cn%252F%7C1563704006; __jsl_clearance=1563705497.101|0|JIDvINurG1IGefblsGH9PKl1us4%3D;
      ```

    * ```css
      __jsluid_h=0503d74e98d9754c18d08f5fb2f334e5; __jsl_clearance=1563705497.101|0|JIDvINurG1IGefblsGH9PKl1us4%3D; JSESSIONID=5B030449E04F414F99E3B3C9124AD4AD-n1:4
      ```

      

    * __jsluid_h 第一次访问的时候服务器设置
    * __jsl_clearance 是有第一次访问返回js动态生成
    * JSESSIONID 第二次正常访问服务端设置 (第二次访问必须携带__jsl_clearance)